{"cells":[{"cell_type":"markdown","source":["# Sentiment Classification with RNN"],"metadata":{"id":"e5VA7dnjAlCT"}},{"cell_type":"markdown","source":["We implement a RNN-based Sentiment Classifier for the imdb movie reviews dataset."],"metadata":{"id":"5F3Q0lNOAovp"}},{"cell_type":"markdown","source":["## Dependencies and Parameters"],"metadata":{"id":"ZnXAl7dLArIJ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17702,"status":"ok","timestamp":1670892884837,"user":{"displayName":"Irene Huang","userId":"03667641878645753525"},"user_tz":300},"id":"gA01ReCTbBOf","outputId":"eba4406d-f9a6-4fa7-91bf-68bcc589408d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import keras\n","from keras.datasets import imdb\n","from google.colab import drive\n","from datetime import datetime\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zvsrC6pt9YPZ"},"outputs":[],"source":["# Define the parameters\n","VOCAB_SIZE = 10000 #88584 #this is length of the word2id dictionary\n","EMBEDDING_SIZE = 32\n","HIDDEN_DIM = 200\n","NUM_LAYERS = 1\n","MAX_LEN = 500\n","NUM_SAMPLES = 25000\n","TEST_SIZE = round(0.15 * NUM_SAMPLES)\n","NUM_EPOCH = 10\n","CALLBACK = True"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WLpLvh2BpX0"},"outputs":[],"source":["SAVE_PATH = f\"/content/drive/MyDrive/models/rnn-{EMBEDDING_SIZE}-{HIDDEN_DIM}-{NUM_LAYERS}-{MAX_LEN}-{NUM_SAMPLES}-{NUM_EPOCH}-{VOCAB_SIZE}\"\n","LOGDIR = SAVE_PATH + '/logs/'\n","print(SAVE_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kuglJTYjIMua"},"outputs":[],"source":["!mkdir $SAVE_PATH"]},{"cell_type":"markdown","source":["## Loading and Preprocessing the Data"],"metadata":{"id":"mvDYOVMq_gkB"}},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":6297,"status":"ok","timestamp":1670892928544,"user":{"displayName":"Irene Huang","userId":"03667641878645753525"},"user_tz":300},"id":"UQ-RdhJjbdwM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d55dd250-118d-4255-db22-ca9f970a164b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n","Loaded dataset with 25000 training samples, 25000 test samples\n"]}],"source":["vocabulary_size = VOCAB_SIZE\n","start_char = 1\n","oov_char = 2\n","index_from = 3\n","\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size, start_char=start_char, oov_char=oov_char, index_from=index_from)\n","print('Loaded dataset with {} training samples, {} test samples'.format(len(X_train), len(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1670892928544,"user":{"displayName":"Irene Huang","userId":"03667641878645753525"},"user_tz":300},"id":"Ra3UvL8CbeKZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b0beb5c-362c-4715-c347-2c7f9b11b40d"},"outputs":[{"output_type":"stream","name":"stdout","text":["---review---\n","[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n","---label---\n","1\n"]}],"source":["# inspect sample\n","print('---review---')\n","print(X_train[0])\n","print('---label---')\n","print(y_train[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670892928544,"user":{"displayName":"Irene Huang","userId":"03667641878645753525"},"user_tz":300},"id":"ytFdnZdlbh6H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"94aed1d6-a9c9-4a6b-dbdb-d5ec2a5a5710"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1641221/1641221 [==============================] - 0s 0us/step\n","88586\n","---review with words---\n","['[START]', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story', 'direction', \"everyone's\", 'really', 'suited', 'the', 'part', 'they', 'played', 'and', 'you', 'could', 'just', 'imagine', 'being', 'there', 'robert', '[OOV]', 'is', 'an', 'amazing', 'actor', 'and', 'now', 'the', 'same', 'being', 'director', '[OOV]', 'father', 'came', 'from', 'the', 'same', 'scottish', 'island', 'as', 'myself', 'so', 'i', 'loved', 'the', 'fact', 'there', 'was', 'a', 'real', 'connection', 'with', 'this', 'film', 'the', 'witty', 'remarks', 'throughout', 'the', 'film', 'were', 'great', 'it', 'was', 'just', 'brilliant', 'so', 'much', 'that', 'i', 'bought', 'the', 'film', 'as', 'soon', 'as', 'it', 'was', 'released', 'for', '[OOV]', 'and', 'would', 'recommend', 'it', 'to', 'everyone', 'to', 'watch', 'and', 'the', 'fly', 'fishing', 'was', 'amazing', 'really', 'cried', 'at', 'the', 'end', 'it', 'was', 'so', 'sad', 'and', 'you', 'know', 'what', 'they', 'say', 'if', 'you', 'cry', 'at', 'a', 'film', 'it', 'must', 'have', 'been', 'good', 'and', 'this', 'definitely', 'was', 'also', '[OOV]', 'to', 'the', 'two', 'little', \"boy's\", 'that', 'played', 'the', '[OOV]', 'of', 'norman', 'and', 'paul', 'they', 'were', 'just', 'brilliant', 'children', 'are', 'often', 'left', 'out', 'of', 'the', '[OOV]', 'list', 'i', 'think', 'because', 'the', 'stars', 'that', 'play', 'them', 'all', 'grown', 'up', 'are', 'such', 'a', 'big', 'profile', 'for', 'the', 'whole', 'film', 'but', 'these', 'children', 'are', 'amazing', 'and', 'should', 'be', 'praised', 'for', 'what', 'they', 'have', 'done', \"don't\", 'you', 'think', 'the', 'whole', 'story', 'was', 'so', 'lovely', 'because', 'it', 'was', 'true', 'and', 'was', \"someone's\", 'life', 'after', 'all', 'that', 'was', 'shared', 'with', 'us', 'all']\n","[START]thisfilmwasjustbrilliantcastinglocationscenerystorydirectioneveryone'sreallysuitedtheparttheyplayedandyoucouldjustimaginebeingthererobert[OOV]isanamazingactorandnowthesamebeingdirector[OOV]fathercamefromthesamescottishislandasmyselfsoilovedthefacttherewasarealconnectionwiththisfilmthewittyremarksthroughoutthefilmweregreatitwasjustbrilliantsomuchthatiboughtthefilmassoonasitwasreleasedfor[OOV]andwouldrecommendittoeveryonetowatchandtheflyfishingwasamazingreallycriedattheenditwassosadandyouknowwhattheysayifyoucryatafilmitmusthavebeengoodandthisdefinitelywasalso[OOV]tothetwolittleboy'sthatplayedthe[OOV]ofnormanandpaultheywerejustbrilliantchildrenareoftenleftoutofthe[OOV]listithinkbecausethestarsthatplaythemallgrownuparesuchabigprofileforthewholefilmbutthesechildrenareamazingandshouldbepraisedforwhattheyhavedonedon'tyouthinkthewholestorywassolovelybecauseitwastrueandwassomeone'slifeafterallthatwassharedwithusall\n","---label---\n","1\n"]}],"source":["# IDs back to words\n","\n","word2id = imdb.get_word_index()\n","inverted_word_index = dict(\n","    (i + index_from, word) for (word, i) in word2id.items()\n",")\n","inverted_word_index[start_char] = \"[START]\"\n","inverted_word_index[oov_char] = \"[OOV]\"\n","print(len(inverted_word_index)) # total number of words\n","\n","print('---review with words---')\n","print([inverted_word_index.get(i, ' ') for i in X_train[0]])\n","print(' '.join([inverted_word_index.get(i, ' ') for i in X_train[0]]))\n","print('---label---')\n","print(y_train[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":894,"status":"ok","timestamp":1670892929435,"user":{"displayName":"Irene Huang","userId":"03667641878645753525"},"user_tz":300},"id":"CsdP6Dpwb4EC","outputId":"d5bd624d-bc0d-45b8-94e5-814903bafd75"},"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum review length: 2697\n","Minimum review length: 14\n","88584\n"]}],"source":["# max and min length review\n","print('Maximum review length: {}'.format(\n","len(max((X_train + X_test), key=len))))\n","print('Minimum review length: {}'.format(\n","len(min((X_test + X_test), key=len))))\n","print(len(word2id))"]},{"cell_type":"markdown","metadata":{"id":"yIdOK53_cEbx"},"source":["### Pad Sequences\n","In order to feed this data into our RNN, all input documents must have the same length. We limit the maximum review length to max_words by truncating longer reviews and padding shorter reviews with a null value, using the pad_sequences() function in Keras."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itNN2Wreb8I5"},"outputs":[],"source":["from keras_preprocessing.sequence import pad_sequences\n","\n","max_words = MAX_LEN\n","X_train = pad_sequences(X_train, maxlen=max_words)\n","X_test = pad_sequences(X_test, maxlen=max_words)"]},{"cell_type":"markdown","metadata":{"id":"-4x98_7GcOlR"},"source":["## Creating the RNN Model Instance"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1670452452080,"user":{"displayName":"Joyce Yuan","userId":"03264865062006359433"},"user_tz":300},"id":"PipH4hdLcLQn","outputId":"681db551-6c55-4b3f-83a9-f1300fec8d95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 500, 32)           320000    \n","                                                                 \n"," lstm (LSTM)                 (None, 200)               186400    \n","                                                                 \n"," dense (Dense)               (None, 1)                 201       \n","                                                                 \n","=================================================================\n","Total params: 506,601\n","Trainable params: 506,601\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["from keras import Sequential\n","from keras.layers import Embedding, LSTM, Dense, Dropout\n","\n","embedding_size=EMBEDDING_SIZE\n","model=Sequential()\n","model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n","model.add(LSTM(HIDDEN_DIM))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","print(model.summary())"]},{"cell_type":"markdown","metadata":{"id":"_YjZvStEFr8e"},"source":["## Callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KV1cupFFFrMZ"},"outputs":[],"source":["class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n","  def __init__(self):\n","    self.train_batch = []\n","    self.train_losses = {}\n","    self.test_batch = []\n","    self.test_losses = {}\n","    self.test_accuracy_batch = []\n","    self.test_accuracy = {}\n","    self.train_accuracy_batch = []\n","    self.train_accuracy = {}\n","    \n","  def on_train_batch_end(self, batch, logs=None):\n","    self.train_batch.append(logs[\"loss\"])\n","    self.train_accuracy_batch.append(logs[\"accuracy\"])\n","\n","  def on_test_batch_end(self, batch, logs=None):\n","    self.test_batch.append(logs[\"loss\"])\n","    self.test_accuracy_batch.append(logs[\"accuracy\"])\n","\n","  def on_epoch_end(self, epoch, logs=None):\n","    self.train_losses[epoch] = self.train_batch\n","    self.test_losses[epoch] = self.test_batch\n","    self.train_accuracy[epoch] = self.train_accuracy_batch\n","    self.test_accuracy[epoch] = self.test_accuracy_batch\n","    self.train_batch = []\n","    self.test_batch = []\n","    self.train_accuracy_batch = []\n","    self.test_accuracy_batch = []\n","    print(\"The average loss for epoch {} is {:7.2f} \".format(epoch, logs[\"loss\"]))\n","\n","loss_and_error = LossAndErrorPrintingCallback()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2dI4UiJGHFw"},"outputs":[],"source":["import time\n","\n","class TimeCallback(keras.callbacks.Callback):\n","    def __init__(self):\n","        self.times = []\n","        # use this value as reference to calculate cummulative time taken\n","        self.timetaken = time.time()\n","        self.batch_times = {}\n","        self.epoch_times = {}\n","        self.total_time = time.time()\n","        \n","    def on_train_batch_end(self, batch, logs=None):\n","      self.times.append(time.time() - self.timetaken)\n","      self.timetaken = time.time()\n","\n","    def on_epoch_end(self,epoch,logs = {}):\n","        self.batch_times[epoch] = self.times\n","        self.epoch_times[epoch] = sum(self.times)\n","\n","        #reset variables\n","        self.times = []\n","        self.timetaken = time.time()\n","\n","        print(f\"it took {(epoch,time.time() - self.timetaken)} time\")\n","\n","    def on_train_end(self,logs={}):\n","      self.total_time = time.time() - self.total_time\n","\n","time_callback = TimeCallback()"]},{"cell_type":"markdown","metadata":{"id":"dI7yAreocz7a"},"source":["## Training\n","We use the binary cross-entropy loss function, Adam optimizer, and accuracy metric."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXgw_2yacs7w"},"outputs":[],"source":["model.compile(loss='binary_crossentropy', \n","             optimizer='adam', \n","             metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arnTAq8Kc2Y3"},"outputs":[],"source":["batch_size = 64\n","test_size = TEST_SIZE\n","num_epochs = NUM_EPOCH\n","samples = NUM_SAMPLES\n","\n","X_test, y_test = X_test[:test_size], y_test[:test_size]\n","X_train2, y_train2 = X_train[:samples], y_train[:samples]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1670452452087,"user":{"displayName":"Joyce Yuan","userId":"03264865062006359433"},"user_tz":300},"id":"Eg3zkWve5wdU","outputId":"b965b367-3e95-49d9-905f-8646ff3a6d91"},"outputs":[{"output_type":"stream","name":"stdout","text":["[1 0 0 ... 0 1 0]\n"]}],"source":["print(y_train2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gbhdzpGHA64h"},"outputs":[],"source":["from datetime import datetime\n","import tensorflow as tf\n","\n","callbacks = []\n","logdir = LOGDIR\n","\n","if CALLBACK:\n","\n","  my_callbacks = [\n","      tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5'),\n","      tf.keras.callbacks.TensorBoard(log_dir=logdir),\n","      tf.keras.callbacks.CSVLogger(logdir + 'logs.csv', append=True, separator=','),\n","      time_callback,\n","      loss_and_error\n","  ]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsDgWPU9A-pZ","executionInfo":{"status":"ok","timestamp":1670456564468,"user_tz":300,"elapsed":58077,"user":{"displayName":"Joyce Yuan","userId":"03264865062006359433"}},"outputId":"69673ae5-1725-4a1b-c4bc-7a218e323179"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","391/391 [==============================] - ETA: 0s - loss: 0.5165 - accuracy: 0.7390it took (0, 9.5367431640625e-07) time\n","The average loss for epoch 0 is    0.52 \n","391/391 [==============================] - 429s 1s/step - loss: 0.5165 - accuracy: 0.7390 - val_loss: 0.3401 - val_accuracy: 0.8589\n","Epoch 2/10\n","391/391 [==============================] - ETA: 0s - loss: 0.3068 - accuracy: 0.8774it took (1, 1.1920928955078125e-06) time\n","The average loss for epoch 1 is    0.31 \n","391/391 [==============================] - 417s 1s/step - loss: 0.3068 - accuracy: 0.8774 - val_loss: 0.3302 - val_accuracy: 0.8603\n","Epoch 3/10\n","391/391 [==============================] - ETA: 0s - loss: 0.2299 - accuracy: 0.9106it took (2, 1.6689300537109375e-06) time\n","The average loss for epoch 2 is    0.23 \n","391/391 [==============================] - 411s 1s/step - loss: 0.2299 - accuracy: 0.9106 - val_loss: 0.3185 - val_accuracy: 0.8677\n","Epoch 4/10\n","391/391 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9306it took (3, 9.5367431640625e-07) time\n","The average loss for epoch 3 is    0.19 \n","391/391 [==============================] - 410s 1s/step - loss: 0.1869 - accuracy: 0.9306 - val_loss: 0.3586 - val_accuracy: 0.8576\n","Epoch 5/10\n","391/391 [==============================] - ETA: 0s - loss: 0.1444 - accuracy: 0.9464it took (4, 1.430511474609375e-06) time\n","The average loss for epoch 4 is    0.14 \n","391/391 [==============================] - 408s 1s/step - loss: 0.1444 - accuracy: 0.9464 - val_loss: 0.4928 - val_accuracy: 0.7827\n","Epoch 6/10\n","391/391 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9631it took (5, 1.1920928955078125e-06) time\n","The average loss for epoch 5 is    0.11 \n","391/391 [==============================] - 407s 1s/step - loss: 0.1063 - accuracy: 0.9631 - val_loss: 0.4512 - val_accuracy: 0.8581\n","Epoch 7/10\n","391/391 [==============================] - ETA: 0s - loss: 0.0724 - accuracy: 0.9774it took (6, 1.6689300537109375e-06) time\n","The average loss for epoch 6 is    0.07 \n","391/391 [==============================] - 407s 1s/step - loss: 0.0724 - accuracy: 0.9774 - val_loss: 0.5241 - val_accuracy: 0.8355\n","Epoch 8/10\n","391/391 [==============================] - ETA: 0s - loss: 0.0864 - accuracy: 0.9722it took (7, 1.1920928955078125e-06) time\n","The average loss for epoch 7 is    0.09 \n","391/391 [==============================] - 408s 1s/step - loss: 0.0864 - accuracy: 0.9722 - val_loss: 0.5965 - val_accuracy: 0.8469\n","Epoch 9/10\n","391/391 [==============================] - ETA: 0s - loss: 0.0491 - accuracy: 0.9842it took (8, 1.1920928955078125e-06) time\n","The average loss for epoch 8 is    0.05 \n","391/391 [==============================] - 407s 1s/step - loss: 0.0491 - accuracy: 0.9842 - val_loss: 0.6646 - val_accuracy: 0.8560\n","Epoch 10/10\n","391/391 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9864it took (9, 1.1920928955078125e-06) time\n","The average loss for epoch 9 is    0.04 \n","391/391 [==============================] - 407s 1s/step - loss: 0.0426 - accuracy: 0.9864 - val_loss: 0.6722 - val_accuracy: 0.8509\n"]}],"source":["history = model.fit(X_train2, y_train2, validation_data=(X_test, y_test), batch_size=batch_size, epochs=num_epochs, callbacks=my_callbacks)"]},{"cell_type":"markdown","metadata":{"id":"j0be2TvW0l1c"},"source":["## Evaluating"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xU-4X_Q3c8O8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670456587929,"user_tz":300,"elapsed":23501,"user":{"displayName":"Joyce Yuan","userId":"03264865062006359433"}},"outputId":"53bb284a-d0a4-47de-b1e9-60a2eab7b1ac"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'loss': [0.5164880156517029, 0.3067905008792877, 0.2298823595046997, 0.18688984215259552, 0.14443299174308777, 0.10630108416080475, 0.0724228248000145, 0.08635959029197693, 0.04910784214735031, 0.04260509833693504], 'accuracy': [0.7390000224113464, 0.8773599863052368, 0.9106000065803528, 0.9305599927902222, 0.9463599920272827, 0.9631199836730957, 0.9774399995803833, 0.9721999764442444, 0.9842000007629395, 0.9863600134849548], 'val_loss': [0.34014835953712463, 0.330218106508255, 0.3185320496559143, 0.35855311155319214, 0.49280136823654175, 0.4512496888637543, 0.5241455435752869, 0.5965407490730286, 0.6645947098731995, 0.6722164750099182], 'val_accuracy': [0.8589333295822144, 0.8602666854858398, 0.867733359336853, 0.8575999736785889, 0.7826666831970215, 0.8581333160400391, 0.8354666829109192, 0.8469333052635193, 0.8560000061988831, 0.850933313369751]}\n","Test accuracy: 0.850933313369751\n"]}],"source":["# evaluate \n","print(history.history)\n","scores = model.evaluate(X_test, y_test, verbose=0)\n","print('Test accuracy:', scores[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bsr6lOrI-nEX"},"outputs":[],"source":["print(f'Total time = {time_callback.total_time}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oue_K9Sdioq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670456590634,"user_tz":300,"elapsed":2707,"user":{"displayName":"Joyce Yuan","userId":"03264865062006359433"}},"outputId":"14de2ba2-9b9a-4407-ded6-33b0e56c0d45"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Found untraced functions such as lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"]}],"source":["# save\n","PATH = SAVE_PATH\n","model.save(SAVE_PATH)"]},{"cell_type":"markdown","metadata":{"id":"NHmuBw_OvtCg"},"source":["## Loading the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XAmUQAexhrWe"},"outputs":[],"source":["# loading the saved model\n","LOADPATH = \"/content/drive/MyDrive/models/rnn-32-100-1-500-5000-5-new\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":461,"status":"ok","timestamp":1670443980251,"user":{"displayName":"Joyce Yuan","userId":"03264865062006359433"},"user_tz":300},"id":"IzgMQJHgQEeG","outputId":"415d43c9-9563-41b5-a429-844c681701b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["acc_vs_batch.png\t logs\t\t\t    time_per_epoch.csv\n","acc_vs_epoch.png\t loss_vs_batch.png\t    train_losses_per_batch.csv\n","assets\t\t\t saved_model.pb\t\t    variables\n","cumul_time_vs_batch.png  test_losses_per_batch.csv\n","keras_metadata.pb\t time_per_batch.csv\n"]}],"source":["!ls $LOADPATH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1i59tWAPVYI"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","new_model = keras.models.load_model(LOADPATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArlBMMAFhsxW"},"outputs":[],"source":["# if you need to process data again\n","vocabulary_size = 88584\n","start_char = 1\n","oov_char = 2\n","index_from = 3\n","max_words = 100\n","samples = 5000\n","test_size = round(0.15 * samples)\n","\n","(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size, start_char=start_char, oov_char=oov_char, index_from=index_from)\n","X_train = pad_sequences(X_train, maxlen=max_words)\n","X_test = pad_sequences(X_test, maxlen=max_words)\n","\n","X_test, y_test = X_test[:test_size], y_test[:test_size]\n","X_train2, y_train2 = X_train[:samples], y_train[:samples]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S_k1SMA4grI2"},"outputs":[],"source":["scores = new_model.evaluate(X_test, y_test, verbose=0)\n","print('Test accuracy:', scores[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QYzA3ZnLPw_H"},"outputs":[],"source":["(X_load_train, y_load_train), (X_load_test, y_load_test) = imdb.load_data(num_words = vocabulary_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bqw2AcwpRUnm"},"outputs":[],"source":["print(X_load_train[0])\n","output = new_model(tf.constant(pad_sequences(X_load_test[0:5], maxlen=max_words), dtype=np.float32))\n","print(output)\n","print(y_test[0:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1T1J0JddAj_"},"outputs":[],"source":["# testing on random inputs\n","inputs = [\"i hate this movie\", \"this sucks\", \"i never want to watch this again\"]\n","seqs = [[word2id[i] for i in inp.split(' ')] for inp in inputs ]\n","print(seqs)\n","output_bad = new_model(tf.constant(pad_sequences(seqs, maxlen=max_words), dtype=np.float32))\n","print(output_bad)"]},{"cell_type":"markdown","metadata":{"id":"-88jsM2ev85N"},"source":["## Plotting Using the Model History\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPd6W9XqwHPT"},"outputs":[],"source":["# CSV Logger has all this\n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PHplEO5Pfh8K"},"outputs":[],"source":["print(history.history.keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3vkpQdprwMe8"},"outputs":[],"source":["# summarize history for accuracy\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gBmZa7tawVW-"},"outputs":[],"source":["# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pTq_3YQryNSd"},"outputs":[],"source":["for loss_type in history.history.keys():\n","  np.savetxt(f\"{loss_type}.txt\", np.array(history.history[loss_type]), delimiter=\",\")\n","  print(np.array(history.history[loss_type]))"]},{"cell_type":"markdown","metadata":{"id":"I22A84Ky8eO7"},"source":["## Saving the Info from the Callbacks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CEodftGjCujE"},"outputs":[],"source":["# remove current logs if exists\n","import os\n","for file in ['train_losses_per_batch.csv', 'test_losses_per_batch.csv', 'time_per_epoch.csv', 'time_per_batch.csv', 'log.csv']:\n","  try:\n","    os.remove(file)\n","  except:\n","    continue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AD6a9aW3M73G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670456590866,"user_tz":300,"elapsed":14,"user":{"displayName":"Joyce Yuan","userId":"03264865062006359433"}},"outputId":"cf27cc03-e523-4a54-b723-a9d6fb6baf78"},"outputs":[{"output_type":"stream","name":"stdout","text":["{0: [0.20284655690193176, 0.2619050145149231, 0.28528931736946106, 0.2870345413684845, 0.30299824476242065, 0.31459352374076843, 0.31224605441093445, 0.29143571853637695, 0.2943876087665558, 0.30005109310150146, 0.3015383780002594, 0.30754032731056213, 0.3157164752483368, 0.3220962584018707, 0.31983014941215515, 0.3309316039085388, 0.3299590051174164, 0.33018290996551514, 0.34008970856666565, 0.33774691820144653, 0.33699968457221985, 0.3347349762916565, 0.33153796195983887, 0.3256082236766815, 0.33024701476097107, 0.32964199781417847, 0.3301847577095032, 0.33240824937820435, 0.33125990629196167, 0.3304252326488495, 0.3295424282550812, 0.3263230621814728, 0.3243754208087921, 0.3240882158279419, 0.3280411660671234, 0.326635479927063, 0.32752570509910583, 0.32603493332862854, 0.3252958357334137, 0.3280918002128601, 0.32883554697036743, 0.3303927779197693, 0.3281922936439514, 0.3281281888484955, 0.3263099193572998, 0.3290502727031708, 0.32868653535842896, 0.3284500539302826, 0.33194756507873535, 0.33394089341163635, 0.3344840705394745, 0.33481499552726746, 0.33388757705688477, 0.33628594875335693, 0.33518528938293457, 0.3364886939525604, 0.3386828899383545, 0.3415250778198242, 0.34014835953712463], 1: [0.23594526946544647, 0.26820048689842224, 0.28870320320129395, 0.2972637116909027, 0.31608718633651733, 0.3374671936035156, 0.3236227333545685, 0.31091123819351196, 0.31587812304496765, 0.30835050344467163, 0.3055274784564972, 0.3077852129936218, 0.311871200799942, 0.3158734440803528, 0.31124699115753174, 0.31446394324302673, 0.3144000172615051, 0.31523221731185913, 0.32044485211372375, 0.3222661018371582, 0.3219292461872101, 0.3220773935317993, 0.31795504689216614, 0.3141044080257416, 0.3159365952014923, 0.31514772772789, 0.31427842378616333, 0.3138309121131897, 0.3116282522678375, 0.3117312788963318, 0.3155294954776764, 0.3151087462902069, 0.3152720332145691, 0.31594809889793396, 0.3175422251224518, 0.31593114137649536, 0.315684050321579, 0.31675538420677185, 0.31684795022010803, 0.3213522136211395, 0.32168832421302795, 0.3218742311000824, 0.3213888704776764, 0.3209049701690674, 0.32053905725479126, 0.3210604786872864, 0.3209337592124939, 0.32071682810783386, 0.32304462790489197, 0.3229290843009949, 0.3233453631401062, 0.3237568438053131, 0.3241620659828186, 0.3261505365371704, 0.3258022367954254, 0.32803988456726074, 0.3292114734649658, 0.33084434270858765, 0.330218106508255], 2: [0.2224178910255432, 0.26746073365211487, 0.27332672476768494, 0.2619669735431671, 0.2677305340766907, 0.2862631380558014, 0.286073237657547, 0.2780534029006958, 0.2876785695552826, 0.2828337550163269, 0.2856581211090088, 0.2869061529636383, 0.29145944118499756, 0.29677993059158325, 0.29395025968551636, 0.29871854186058044, 0.29698827862739563, 0.2964252829551697, 0.3045388460159302, 0.3043566346168518, 0.30386990308761597, 0.30551877617836, 0.30230090022087097, 0.29786646366119385, 0.3015362024307251, 0.300821989774704, 0.3029187023639679, 0.30277153849601746, 0.2996615767478943, 0.30051949620246887, 0.30463260412216187, 0.3018955588340759, 0.2990579307079315, 0.29971325397491455, 0.30156880617141724, 0.2996785044670105, 0.3013453483581543, 0.30272236466407776, 0.30236804485321045, 0.3060924708843231, 0.3060275614261627, 0.3069334030151367, 0.3062060475349426, 0.30484411120414734, 0.30435609817504883, 0.3066357672214508, 0.30555567145347595, 0.3066103458404541, 0.3117159903049469, 0.31183886528015137, 0.3118785619735718, 0.3128206133842468, 0.3122756779193878, 0.3159569203853607, 0.31558409333229065, 0.31592506170272827, 0.317187637090683, 0.31982359290122986, 0.3185320496559143], 3: [0.2058182656764984, 0.32299309968948364, 0.3109581470489502, 0.28478771448135376, 0.30348461866378784, 0.31928882002830505, 0.3297232985496521, 0.3136269450187683, 0.3223811686038971, 0.3268086016178131, 0.32437750697135925, 0.3243189752101898, 0.32418543100357056, 0.32939836382865906, 0.3315211832523346, 0.3415359854698181, 0.3413596749305725, 0.3425796627998352, 0.3494102656841278, 0.34954947233200073, 0.3490385413169861, 0.34836411476135254, 0.345127135515213, 0.3407320976257324, 0.34501758217811584, 0.3448176681995392, 0.35010215640068054, 0.3494635224342346, 0.3451402485370636, 0.3443832993507385, 0.3457380533218384, 0.34061914682388306, 0.3378792405128479, 0.33856889605522156, 0.341537207365036, 0.3399026691913605, 0.34278443455696106, 0.3426389694213867, 0.3407442569732666, 0.3473210036754608, 0.34763696789741516, 0.3484911620616913, 0.34637022018432617, 0.3456137478351593, 0.34478798508644104, 0.34843528270721436, 0.3456431031227112, 0.3459782600402832, 0.3512692451477051, 0.35303568840026855, 0.35260894894599915, 0.35285502672195435, 0.35172033309936523, 0.35564133524894714, 0.35453152656555176, 0.3544130027294159, 0.3571360111236572, 0.35957008600234985, 0.35855311155319214], 4: [0.2556098997592926, 0.4647277593612671, 0.43477001786231995, 0.4326292872428894, 0.43701332807540894, 0.44383183121681213, 0.4601384997367859, 0.4352853298187256, 0.4365195631980896, 0.4390038549900055, 0.4419212341308594, 0.4471306800842285, 0.4507976770401001, 0.46313387155532837, 0.4712822735309601, 0.4778624475002289, 0.4963591992855072, 0.49748775362968445, 0.5009640455245972, 0.5005260109901428, 0.5010895729064941, 0.4972909092903137, 0.49672967195510864, 0.4914880096912384, 0.49508243799209595, 0.4917777478694916, 0.4908353388309479, 0.48838046193122864, 0.48672086000442505, 0.48356926441192627, 0.48662760853767395, 0.4817865192890167, 0.48015037178993225, 0.4825971722602844, 0.48327991366386414, 0.48340702056884766, 0.4845332205295563, 0.4831725060939789, 0.4823649823665619, 0.48506250977516174, 0.4858973026275635, 0.48678770661354065, 0.4839910566806793, 0.48329687118530273, 0.48411881923675537, 0.4886654019355774, 0.4849766790866852, 0.48469996452331543, 0.4868680238723755, 0.4897066354751587, 0.48885679244995117, 0.48872408270835876, 0.486333429813385, 0.4892095625400543, 0.4872739315032959, 0.48906630277633667, 0.4911308288574219, 0.49296727776527405, 0.49280136823654175], 5: [0.29437342286109924, 0.4123684763908386, 0.4130837917327881, 0.3834072947502136, 0.39579862356185913, 0.410735160112381, 0.399810254573822, 0.4060781002044678, 0.42439162731170654, 0.42194199562072754, 0.4080004394054413, 0.4108912944793701, 0.40674400329589844, 0.42169955372810364, 0.41767600178718567, 0.4277345538139343, 0.4236511290073395, 0.42074263095855713, 0.4248065948486328, 0.4325447678565979, 0.43630295991897583, 0.43897178769111633, 0.4416101574897766, 0.4378416836261749, 0.44654834270477295, 0.4425678253173828, 0.4451845586299896, 0.4416957497596741, 0.43793410062789917, 0.43997982144355774, 0.44354572892189026, 0.43772226572036743, 0.43560975790023804, 0.43374860286712646, 0.43619880080223083, 0.43313443660736084, 0.43607550859451294, 0.43349435925483704, 0.4320842921733856, 0.43696483969688416, 0.4371080696582794, 0.43635523319244385, 0.4374469816684723, 0.43472835421562195, 0.43521246314048767, 0.43783262372016907, 0.43591323494911194, 0.43882182240486145, 0.44336748123168945, 0.4433155953884125, 0.43928706645965576, 0.4385944604873657, 0.4386065602302551, 0.44217413663864136, 0.44159817695617676, 0.4436075985431671, 0.44831088185310364, 0.45328325033187866, 0.4512496888637543], 6: [0.39061135053634644, 0.6604304313659668, 0.6385056972503662, 0.5896084904670715, 0.5753788948059082, 0.5798888206481934, 0.5474362373352051, 0.5336118936538696, 0.56119704246521, 0.5457908511161804, 0.5348074436187744, 0.5397335886955261, 0.5291509628295898, 0.528283417224884, 0.516777753829956, 0.522390604019165, 0.5200313329696655, 0.5243823528289795, 0.5267658829689026, 0.5363894701004028, 0.5370885133743286, 0.5352882742881775, 0.536270260810852, 0.5330324172973633, 0.5342969298362732, 0.5304870009422302, 0.5297672748565674, 0.5267239212989807, 0.522706925868988, 0.5216113328933716, 0.5240048170089722, 0.5210553407669067, 0.518929660320282, 0.5184178352355957, 0.5193910598754883, 0.51869797706604, 0.5220850706100464, 0.5173776745796204, 0.5126997828483582, 0.5180054903030396, 0.5172331929206848, 0.5163910984992981, 0.5149223804473877, 0.5121460556983948, 0.5128763318061829, 0.5151143074035645, 0.5117415189743042, 0.5149096250534058, 0.518159031867981, 0.5192065238952637, 0.5182769298553467, 0.5173575282096863, 0.5158142447471619, 0.5169428586959839, 0.515631377696991, 0.5184089541435242, 0.5204868912696838, 0.526237964630127, 0.5241455435752869], 7: [0.45713916420936584, 0.6340466141700745, 0.6541146636009216, 0.6569390892982483, 0.6626574993133545, 0.6781167984008789, 0.6549765467643738, 0.638238787651062, 0.6465191841125488, 0.6195594072341919, 0.6084988117218018, 0.6251163482666016, 0.6005839109420776, 0.6066333651542664, 0.5936081409454346, 0.6135736703872681, 0.6031280755996704, 0.603850781917572, 0.6030977964401245, 0.6100077629089355, 0.6131321787834167, 0.6157128810882568, 0.6125072240829468, 0.6108168959617615, 0.6179136037826538, 0.6106192469596863, 0.6095351576805115, 0.6037706136703491, 0.5979503393173218, 0.5973093509674072, 0.5949629545211792, 0.589484453201294, 0.585808515548706, 0.5852083563804626, 0.5890945196151733, 0.5851160883903503, 0.5895702838897705, 0.5826608538627625, 0.5786361694335938, 0.5900431275367737, 0.5925084352493286, 0.5889078974723816, 0.5892534255981445, 0.5878309011459351, 0.5887592434883118, 0.5911123156547546, 0.5865916013717651, 0.5904572010040283, 0.5938515067100525, 0.592206597328186, 0.5895898342132568, 0.5889464020729065, 0.5882745981216431, 0.589893102645874, 0.5895454287528992, 0.5938556790351868, 0.5965554714202881, 0.6007925868034363, 0.5965407490730286], 8: [0.3944806456565857, 0.5768552422523499, 0.6284993290901184, 0.6456631422042847, 0.6325317621231079, 0.6904306411743164, 0.6962270736694336, 0.6621161699295044, 0.6937969923019409, 0.6807535290718079, 0.6531545519828796, 0.6621208786964417, 0.6482146978378296, 0.6629964113235474, 0.656464695930481, 0.686194658279419, 0.6683448553085327, 0.6669079661369324, 0.6735994219779968, 0.6793631911277771, 0.6862539649009705, 0.6825885772705078, 0.6797441840171814, 0.6791194081306458, 0.6815870404243469, 0.6703821420669556, 0.6692025065422058, 0.6655641198158264, 0.662164032459259, 0.6612554788589478, 0.6657079458236694, 0.656009316444397, 0.6473605632781982, 0.6562541127204895, 0.6629081964492798, 0.6594130396842957, 0.6593214273452759, 0.6509963870048523, 0.6470351815223694, 0.6580476760864258, 0.6591149568557739, 0.6543740034103394, 0.6539843082427979, 0.6486366391181946, 0.6455592513084412, 0.6510650515556335, 0.6477822065353394, 0.6556270718574524, 0.6622017621994019, 0.6641549468040466, 0.6601964235305786, 0.6562032699584961, 0.654500424861908, 0.6575676798820496, 0.655839741230011, 0.657659113407135, 0.665585994720459, 0.6679912209510803, 0.6645947098731995], 9: [0.3812863826751709, 0.5200884938240051, 0.5832304358482361, 0.5891613960266113, 0.6309954524040222, 0.6660122871398926, 0.6660882234573364, 0.6377227902412415, 0.6753116250038147, 0.6830981373786926, 0.6483118534088135, 0.6572847962379456, 0.6519711017608643, 0.6689494252204895, 0.6635956168174744, 0.6905008554458618, 0.6827638149261475, 0.6891574263572693, 0.6903800964355469, 0.6902447938919067, 0.6925860643386841, 0.6901237964630127, 0.6868752837181091, 0.6842868328094482, 0.6883682012557983, 0.6864324808120728, 0.6844609379768372, 0.6839348673820496, 0.6827298998832703, 0.6833010315895081, 0.6826368570327759, 0.6741706132888794, 0.6635448336601257, 0.6652953028678894, 0.6699210405349731, 0.6655013561248779, 0.6670464873313904, 0.6592376828193665, 0.6536377668380737, 0.6624932885169983, 0.6622312664985657, 0.6575753688812256, 0.6598656177520752, 0.6566299200057983, 0.65562903881073, 0.6608170866966248, 0.6568188071250916, 0.6662754416465759, 0.6715512275695801, 0.6740076541900635, 0.668841540813446, 0.6656849980354309, 0.6656845808029175, 0.6657071113586426, 0.6644181609153748, 0.6671039462089539, 0.6726369261741638, 0.6754749417304993, 0.6722164750099182]}\n"]}],"source":["# save training and test loss per epoch\n","import csv\n","\n","filename = SAVE_PATH + '/train_losses_per_batch.csv'\n","with open(filename, 'w') as f:\n","  writer = csv.DictWriter(f, fieldnames=[\"Epoch\", \"Training Loss\"])\n","  writer.writeheader()\n","  for epoch in loss_and_error.train_losses.keys():\n","    for loss in loss_and_error.train_losses[epoch]:\n","      data = {\"Epoch\": epoch, \"Training Loss\": loss}\n","      writer.writerow(data)\n","\n","filename = SAVE_PATH + '/test_losses_per_batch.csv'\n","with open(filename, 'w') as f:\n","  writer = csv.DictWriter(f, fieldnames=[\"Epoch\", \"Test Loss\"])\n","  writer.writeheader()\n","  for epoch in loss_and_error.test_losses.keys():\n","    for loss in loss_and_error.test_losses[epoch]:\n","      data = {\"Epoch\": epoch, \"Test Loss\": loss}\n","      writer.writerow(data)\n","\n","filename = SAVE_PATH + '/train_accuracy_per_batch.csv'\n","with open(filename, 'w') as f:\n","  writer = csv.DictWriter(f, fieldnames=[\"Epoch\", \"Train Accuracy\"])\n","  writer.writeheader()\n","  for epoch in loss_and_error.train_accuracy.keys():\n","    for loss in loss_and_error.train_accuracy[epoch]:\n","      data = {\"Epoch\": epoch, \"Train Accuracy\": loss}\n","      writer.writerow(data)\n","\n","filename = SAVE_PATH + '/test_accuracy_per_batch.csv'\n","with open(filename, 'w') as f:\n","  writer = csv.DictWriter(f, fieldnames=[\"Epoch\", \"Test Accuracy\"])\n","  writer.writeheader()\n","  for epoch in loss_and_error.test_accuracy.keys():\n","    for loss in loss_and_error.test_accuracy[epoch]:\n","      data = {\"Epoch\": epoch, \"Test Accuracy\": loss}\n","      writer.writerow(data)\n","\n","print(loss_and_error.test_losses)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Z5f2MbU17ej","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670456590878,"user_tz":300,"elapsed":5,"user":{"displayName":"Joyce Yuan","userId":"03264865062006359433"}},"outputId":"5ae8cd12-c29e-4f7b-d880-457283c1f1f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["{0: 408.1657633781433, 1: 396.4657554626465, 2: 390.7009654045105, 3: 389.960205078125, 4: 387.50929284095764, 5: 386.586377620697, 6: 386.9248082637787, 7: 387.29252099990845, 8: 386.8334963321686, 9: 386.12139916419983}\n","{0: 408.1657633781433, 1: 396.4657554626465, 2: 390.7009654045105, 3: 389.960205078125, 4: 387.50929284095764, 5: 386.586377620697, 6: 386.9248082637787, 7: 387.29252099990845, 8: 386.8334963321686, 9: 386.12139916419983}\n"]}],"source":["# save the time\n","file1 = SAVE_PATH + '/time_per_epoch.csv'\n","file2 = SAVE_PATH + '/time_per_batch.csv'\n","try:\n","    with open(file1, 'w') as csvfile:\n","        writer = csv.DictWriter(csvfile, fieldnames=time_callback.epoch_times.keys())\n","        writer.writeheader()\n","        for data in [time_callback.epoch_times]:\n","            writer.writerow(data)\n","    with open(file2, 'w') as f:\n","      writer = csv.DictWriter(f, fieldnames=[\"Epoch\", \"Batch\", \"Time\"])\n","      writer.writeheader()\n","      for epoch in time_callback.batch_times.keys():\n","        for batch, time in enumerate(time_callback.batch_times[epoch]):\n","          data = {\"Epoch\": epoch, \"Batch\": batch, \"Time\": time}\n","          writer.writerow(data)\n","except IOError:\n","    print(\"I/O error\")\n","\n","print(time_callback.epoch_times)"]},{"cell_type":"markdown","source":["## References\n","\n","- We implemented the model as an adaption of [Sentiment Analysis with RNN](https://github.com/susanli2016/NLP-with-Python/blob/master/Sentiment%20Analysis%20with%20RNN.ipynb)\n","- We created the plots in this notebook referencing [this post on using the model's history for plotting](https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/)"],"metadata":{"id":"lmU7ByT6-OB3"}}],"metadata":{"colab":{"provenance":[{"file_id":"1BOTMme77eokDZI6kgRAR0OAaHBIsowPd","timestamp":1670222047614}],"collapsed_sections":["_YjZvStEFr8e","dI7yAreocz7a","j0be2TvW0l1c","NHmuBw_OvtCg","-88jsM2ev85N","I22A84Ky8eO7","lmU7ByT6-OB3"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}